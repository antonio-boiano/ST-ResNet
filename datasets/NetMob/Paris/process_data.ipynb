{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T14:15:29.724863374Z",
     "start_time": "2023-09-01T14:15:28.598103491Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Paris_Instagram_TrafficMapSeries_20190316_20190531.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Load .npy file\u001B[39;00m\n\u001B[1;32m      4\u001B[0m file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParis_Instagram_TrafficMapSeries_20190316_20190531.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mParis_Instagram_TrafficMapSeries_20190316_20190531.npy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      7\u001B[0m         tensor_traffic_original \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m      9\u001B[0m cdims\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m409\u001B[39m,\u001B[38;5;241m346\u001B[39m]\n",
      "File \u001B[0;32m~/NetMOB23/ST-ResNet/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    283\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    284\u001B[0m     )\n\u001B[0;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Paris_Instagram_TrafficMapSeries_20190316_20190531.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load .npy file\n",
    "file_path = \"Paris_Instagram_TrafficMapSeries_20190316_20190531.npy\"\n",
    "\n",
    "with open('Paris_Instagram_TrafficMapSeries_20190316_20190531.npy', 'rb') as f:\n",
    "        tensor_traffic_original = np.load(f)\n",
    "\n",
    "cdims=[409,346]\n",
    "\n",
    "# Group tiles to create 1km x 1km macro-tiles (each macro_tile contains 10 cells)\n",
    "pad_x = 10 - cdims[0] % 10\n",
    "pad_y = 10 - cdims[1] % 10\n",
    "\n",
    "tensor_traffic_original = np.pad(tensor_traffic_original, ((0,0), (0,pad_x),(0,pad_y)), 'constant', constant_values = 0)\n",
    "tensor_traffic = tensor_traffic_original.reshape(tensor_traffic_original.shape[0], int((cdims[0] + pad_x) / 10),\n",
    "                                        10, int((cdims[1] + pad_y) / 10), 10).sum(axis=(2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the Data from (1848, 41, 35) to (1848, 2, 41, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1848, 41, 35)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1848, 2, 41, 35)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tensor_traffic.shape)\n",
    "\n",
    "tensor_traffic_reshape = np.reshape(tensor_traffic, (1848, 1, 41, 35))\n",
    "\n",
    "tensor_traffic_reshape_2 = np.concatenate((tensor_traffic_reshape, tensor_traffic_reshape), axis=1)\n",
    "#tensor_traffic_reshape_2=tensor_traffic_reshape\n",
    "#tensor_traffic_reshape_2 = tensor_traffic\n",
    "\n",
    "display(tensor_traffic_reshape_2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import a dataset to extract the time date_arr and take the first 1848 elements same dimensionality of the network dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (4392, 2, 16, 8)\n",
      "date (4392,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import h5py\n",
    "f = h5py.File('/home/antonio/NetMOB23/ST-ResNet/datasets/NetMob/Paris/Paris_Instagram_TrafficMapSeries_20190316_20190531_REDUCED.h5')\n",
    "for ke in f.keys():\n",
    "    print(ke, f[ke].shape)\n",
    "\n",
    "date_arr = np.array(f['date'][:1848])\n",
    "\n",
    "data_arr_b = np.array(f['data'])\n",
    "date_arr_b = np.array(f['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge the 2 dataset and save it in h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (1848, 2, 41, 35)\n",
      "date (1848,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "# Create an h5py file\n",
    "with h5py.File('Paris_Instagram_TrafficMapSeries_20190316_20190531_REDUCED.h5', 'w') as hf:\n",
    "    # Create a dataset inside the file\n",
    "    hf.create_dataset('data', data=tensor_traffic_reshape_2)\n",
    "    hf.create_dataset('date', data=date_arr)\n",
    "    \n",
    "    \n",
    "f = h5py.File('Paris_Instagram_TrafficMapSeries_20190316_20190531_REDUCED.h5')\n",
    "for ke in f.keys():\n",
    "    print(ke, f[ke].shape)\n",
    "    \n",
    "data_arr = np.array(f['data'])\n",
    "date_arr = np.array(f['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T14:18:17.448526015Z",
     "start_time": "2023-09-01T14:18:17.344581820Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (1848, 2, 41, 35)\n",
      "date (1848,)\n",
      "(1848, 41, 35)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "f = h5py.File('/home/antonio/NetMOB23/ST-ResNet/datasets/NetMob/Paris/Paris_Instagram_TrafficMapSeries_20190316_20190531_REDUCED.h5')\n",
    "for ke in f.keys():\n",
    "    print(ke, f[ke].shape)\n",
    "    \n",
    "#data_arr = np.array(f['data'])\n",
    "date_arr = np.array(f['date'])\n",
    "data_arr = np.array(f['data'][:,0,:,:])\n",
    "\n",
    "#data_arr = np.reshape(data_arr, (1848, 1, 41, 35))\n",
    "\n",
    "print(data_arr.shape)\n",
    "\n",
    "with h5py.File('Paris_Instagram_TrafficMapSeries_20190316_20190531_REDUCED_0.h5', 'w') as hf:\n",
    "    # Create a dataset inside the file\n",
    "    hf.create_dataset('data', data=data_arr)\n",
    "    hf.create_dataset('date', data=date_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T14:57:33.339810500Z",
     "start_time": "2023-09-01T14:57:33.064092710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (1848, 2, 41, 35)\n",
      "date (1848,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "f = h5py.File('/home/antonio/NetMOB23/ST-ResNet/datasets/NetMob/Paris/Paris_Instagram_TrafficMapSeries_20190316_20190531_REDUCED.h5')\n",
    "for ke in f.keys():\n",
    "    print(ke, f[ke].shape)\n",
    "    \n",
    "#data_arr = np.array(f['data'])\n",
    "date_arr = np.array(f['date'])\n",
    "data_arr = np.array(f['data'][:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
